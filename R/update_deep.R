#' update method for probit-class
#'
#' @description
#' update method for \code{probit}-object as generated by \code{\link{probit}}.
#'
#'
#' @param object Object of class \code{probit}.
#' @param fixed Update of right hand side of model formula for fixed effects. Defaults to \code{fixed=NULL}, which corresponds to no update.
#' @param random List of updates of right hand side of formulas for the random effects. Defaults to \code{random=NULL}, which corresponds to no update.
#' @param A Transformation matrix of predicted random effects. Defaults to \code{A=NULL}, which corresponds to identity transformation of residing random effects.
#' @param dependence Text string (\code{"marginal" or "joint"}) deciding whether random effects are assumed independent or with a common joint normal distribution. Defaults to \code{dependence=NULL}, which corresponds to no update.
#' @param data Date frame with data on the wide format. Defaults to \code{data=NULL}, which corresponds to \code{data} taken from the call object.
#' @param B Number of simulations in minimization step. Defaults to \code{B=NULL}, which corresponds to \code{B} taken from the call object.
#' @param BB Number of simulations per subject in \code{anova} or \code{update}. Defaults to \code{BB=NULL}, which corresponds to \code{BB} taken from the call object.
#' @param maxit Maximal number of minimization-maximization steps. Defaults to \code{maxit=20}.
#' @param sig.level Significance level at which the iterative stochastic optimization will be stopped. Defaults to \code{sig.level=0.60}.
#' @param verbose Numeric controlling amount of convergence diagnostics. Default: \code{verbose=0} corresponding to no output.
#' @param estimate.models Boolean deciding if model parameters are estimated in \code{update}.
#'
#' @return \code{\link{probit-class}} object.
#'
#' @export
update_deep <- function(object,fixed=NULL,random=NULL,A=NULL,dependence=NULL,data=NULL,B=NULL,BB=NULL,maxit=20,sig.level=0.6,verbose=0,estimate.models=TRUE) {
  # is a deep update to be done?
  deep_update <- !(is.null(fixed)&is.null(random))

  # update fixed effects?
  if (is.null(fixed)) {
    fixed <- formula(object$fixed)
    m.fixed <- object$m.fixed
  } else {
    fixed <- update(formula(object$fixed),fixed)
    m.fixed <- NULL
  }

  # update random effect models
  if (is.null(random)) {random <- object$random} else {
    new_random <- object$random
    random_ii  <- match(sapply(random,function(x){all.vars(x[[2]])}),
                        sapply(new_random,function(x){all.vars(x[[2]])}))
    for (i in 1:length(random_ii)) {
      if (is.na(random_ii[i])) {
        new_random <- c(new_random,random[[i]])
      } else {
        new_random[[random_ii[i]]] <- update(new_random[[random_ii[i]]],random[[i]])
      }
    }
    random <- new_random
  }

  # update values of dependence, B, BB, data?
  if (is.null(dependence)) {dependence <- object$dependence}
  if (is.null(B))          {B <- object$B}
  if (is.null(BB))         {BB <- object$BB}
  if (is.null(data)) {data <- object$data} else {data <- as_tibble(data)}

  # fix dependence
  if (dependence!="marginal") dependence <- "joint"

  # remove non-used random effects
  random <- random[is.element(sapply(random,function(x){all.vars(x[[2]])}),
                              all.vars(delete.response(terms(fixed))))]
  q <- length(random)
  if (q<1) stop("Present implementation assumes at least one random effect")

  # find random effects
  new_random.eff <- sapply(random,function(x){all.vars(x[[2]])})
  old_random.eff <- sapply(object$random,function(x){all.vars(x[[2]])})

  # remove observations with missing explanatory variables
  i <- complete.cases(select(data,any_of(setdiff(
    c(all.vars(fixed),unlist(sapply(random,function(x){all.vars(x)}))),
    c(object$items.interval,object$items.ordinal,new_random.eff)))))
  if (sum(!i)>0) {
    data <- data[i,]
    warning("Remove ",sum(!i)," observations with non-complete explanatory variables. NOTE: This may interact badly with update().")
  }

  # find subjects
  subjects     <- unique(data[[object$subject]])
  old_subjects <- unique(object$data[[object$subject]])

  # initiate mu at model predictions
  old_mu <- matrix(0,length(subjects),ncol(object$mu))
  data.short <- data %>% group_by(!!as.name(object$subject)) %>% slice_head(n=1)
  for (i in 1:ncol(object$mu)) {
    old_mu[,i] <- predict(object$m.random[[i]],newdata=data.short)
  }
  # initiate psi at the mean (doesn't necessarily make any sense)
  old_psi <- matrix(colMeans(object$psi),length(subjects),ncol(object$psi),byrow=TRUE)
  # reset (mu,psi) for known subjects at their present value
  ii <- is.element(subjects,old_subjects)
  old_mu[ii,]  <- object$mu[match(subjects,old_subjects)[ii]]
  old_psi[ii,] <- object$psi[match(subjects,old_subjects)[ii]]

  # make deep update if required
  if (deep_update) {
    # set-up data matrix with predicted random input in old model
    U <- as_tibble(cbind(subjects,old_mu),.name_repair = "minimal")
    names(U) <- c(object$subject,old_random.eff)
    mydata   <- full_join(data,U,by=object$subject)
    for (i in object$items.ordinal) mydata[,i] <- as.numeric(as.matrix(mydata[,i]))
    mydata <- pivot_longer(mydata,
                           all_of(c(object$items.interval,object$items.ordinal)),
                           names_to = object$item.name, values_to = object$response.name)
    mydata[,object$response.name] <- predict_slim(object$m.fixed,mydata)
    mydata <- select(mydata,-all_of(old_random.eff))

    # criterion function for minimization over A in new_mu = old_mu%*%t(A)
    crit <- function(par,return.model=FALSE) {
      # take parameters
#      a <- par[1:q] [-(1:q)]
      A <- matrix(par,length(new_random.eff),length(old_random.eff))

      # set-up predicted random effects
#matrix(a,length(subjects),q,byrow=TRUE)+
      U <- as_tibble(cbind(subjects,old_mu%*%t(A)),
                     .name_repair = "minimal")
      names(U) <- c(object$subject,new_random.eff)

      # fit model
      m.fixed <- biglm::biglm(eval(substitute(update(formula(fixed),y~.),list(y=as.name(object$response.name)))),
                              data=full_join(mydata,U,by=object$subject))

      # return model or deviance
      if (return.model) {
        return(m.fixed)
      } else {
        return(deviance(m.fixed))
      }
    }

    # optimize
    A <- matrix(0,length(new_random.eff),length(old_random.eff))
    A[outer(new_random.eff,old_random.eff,"==")] <- 1
    res <- nlminb(c(A),crit)  #rep(0,q),

    # find predictions and cholesky inverse variances of new random effects
    #a <- res$par[1:q] [-(1:q)]
    A <- matrix(res$par,length(new_random.eff),length(old_random.eff))

#matrix(a,length(subjects),q,byrow=TRUE) +
    mu  <- old_mu%*%t(A)
    psi <- matrix(0,length(subjects),q*(q+1)/2)
    for (s in 1:length(subjects)) {
      tmp <- matrix(0,length(old_random.eff),length(old_random.eff))
      tmp[upper.tri(tmp,diag = TRUE)] <- old_psi[s,]
      tmp <- chol(solve(A%*%solve(t(tmp)%*%tmp)%*%t(A)))
      psi[s,] <- tmp[upper.tri(tmp,diag = TRUE)]
    }

    # estimate new fixed effect model
    m.fixed <- crit(res$par,return.model = TRUE)

    # estimate random effects models
    m.random <- vector("list",length(new_random.eff))
    names(m.random) <- new_random.eff
    data.short <- data %>% group_by(!!as.name(object$subject)) %>% slice_head(n=1)
    U <- as.data.frame(cbind(subjects,mu)); names(U) <- c(object$subject,new_random.eff)
    data.short <- full_join(data.short,U,by=object$subject)
    for (i in 1:q) m.random[[i]] <- lm(random[[i]],data=data.short)

    # linear parametrization matrix Q such that
    # 1) diagonal elements in parameter indexes = cumsum(1:q)
    # 2) Q Psi = matrix(Q%*%Psi,q,q)
    # 3) Q.mu  = matrix(tildeQ%*%mu,q,q*(q+1)/2)
    Q <- matrix(0,q*q,q*(q+1)/2)
    Q[matrix(1:(q*q),q,q)[upper.tri(matrix(0,q,q),diag=TRUE)],] <- diag(nrow=q*(q+1)/2)
    tildeQ <- matrix(aperm(array(Q,dim=c(q,q,q*(q+1)/2)),c(1,3,2)),q*q*(q+1)/2,q)

    # means of random effects
    mean.Z <- matrix(0,length(subjects),q)
    for (i in 1:q) mean.Z[,i] <- predict_slim(m.random[[i]],data.short)

    # estimate Gamma
    hat.var <- matrix(rowMeans(matrix(
      apply(mu-mean.Z,1,function(x){x%*%t(x)}) +
        apply(psi,1,function(x){solve(t(matrix(Q%*%x,q,q))%*%matrix(Q%*%x,q,q))})
      ,q*q,nrow(mu))),q,q)
    if (dependence=="marginal") {
      hat.var[upper.tri(hat.var)] <- 0
      hat.var[lower.tri(hat.var)] <- 0
    }
    Gamma <- chol(solve(hat.var))
    rownames(Gamma) <- colnames(Gamma) <- new_random.eff

    # end of deep update
  } else {
    mu  <- old_mu
    psi <- old_psi
    m.random <- object$m.random
    Gamma <- object$Gamma
  }

  # refit and return
  return(MM_probit(maxit,sig.level,verbose,
                   fixed,object$response.name,object$item.name,object$items.interval,object$items.ordinal,
                   object$subject,random,dependence,
                   m.fixed,object$sigma2,object$eta,m.random=m.random,Gamma=Gamma,
#                   m.fixed=NULL,sigma2=NULL,eta=NULL,m.random=NULL,Gamma=Gamma,
                   mu,psi,
                   B,BB,
                   data,
                   estimate.models=estimate.models))
}


